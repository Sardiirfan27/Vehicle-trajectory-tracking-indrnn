# -*- coding: utf-8 -*-
"""VTP_1.10_LSTM Models for a large dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RYWQTjOmU3_P4Zr7Nd4utDTJjvokoXcM

# Developing models for full dataset
---

Overview on the configuration of all simple models

|Model name|Description| # Car|# vars|# targets|# const_vars| # steps | # futures
|---|:--|:-:|:-:|:-:|:-:|:-:|:-:|

## Import packages
"""

# For general
import matplotlib.pyplot as plt
import numpy as np
import time
plt.rcParams['figure.figsize'] = (8, 6)
from math import sqrt

# For data processing
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# For prediction model
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Bidirectional
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import TimeDistributed
from tensorflow.keras.layers import RepeatVector
from tensorflow.keras.layers import ConvLSTM2D
from tensorflow.keras.layers import MaxPool1D, Conv1D


"""## Load dataset"""

url_1 = 'https://github.com/duonghung86/Vehicle-trajectory-tracking/raw/master/Data/NGSIM/0750_0805_us101_smoothed_11_.zip'
zip_path = tf.keras.utils.get_file(origin=url_1, fname=url_1.split('/')[-1], extract=True)
csv_path = zip_path.replace('zip','csv')
csv_path

"""Let's take a glance at the data. Here are the first few rows:"""

df = pd.read_csv(csv_path)
df.info()
df.head()

"""Next look at the statistics of the dataset:"""

df.describe().transpose().round(3)

df.columns

#  keep only columns that are useful for now
kept_cols = ['Vehicle_ID', 'Frame_ID', 'Total_Frames', 'Local_X','Local_Y',
             'v_Length', 'v_Width', 'v_Vel', 'v_Acc', 'Lane_ID']
df = df[kept_cols]
df.head()

'the number of vehicles is {}'.format(len(df.Vehicle_ID.unique()))

"""# Model 1

### Constant values
"""

n_steps = 6
n_future = 3
n_features = len(df)
series_feature_names = ['Local_X','v_Vel','Local_Y', 'v_Acc', 'Lane_ID']
target_names = ['Local_X','v_Vel']
n_labels = len(target_names)
vehicle_ids = df.Vehicle_ID.unique()

"""## Data preparation

### `series2seq`: Function that return sequence input and output for one object

**Arguments**:

- data: Sequence of observations as a Pandas dataframe.
- n_in: Number of lag observations as input (X).
- n_out: Number of observations as output (y).
- **series_features**: names of series features
- labels: name of target variables
- dropnan: Boolean whether or not to drop rows with NaN values.
    
**Returns**:
- X: Feature Pandas DataFrame
- y: Label Pandas dataframe
"""

# Test data frame
# data set of the vehicle #2
df_test = df[df.Vehicle_ID==vehicle_ids[0]].copy()
df_test.info()
df_test.head()

def series2seq(data, n_in=1, n_out=1,labels=None,series_features=None, show_result=False):
    dat = data.copy()
    
    features = dat.columns
    targets = labels
    cols, names = list(), list()
    # input sequence (t-n, ... t-1)

    for i in range(n_in, 0, -1):
        cols.append(dat[series_features].shift(i))
        names += ['{}(t-{})'.format(j, i) for j in series_features]
    # forecast sequence (t, t+1, ... t+n) for selected labels
    #print(targets)
    for i in range(0, n_out):
        cols.append(dat[targets].shift(-i))
        names += ['{}(t+{})'.format(j, i) for j in targets]
    # put it all together
    agg = pd.concat(cols, axis=1).dropna()
    agg.columns = names
    # concatenate with constant features

    X = agg.iloc[:,:len(series_features)*n_in]
    X = pd.concat([X,dat.drop(columns=series_features)], axis=1).dropna()
    y = agg.iloc[:,len(series_features)*n_in:].copy()
    if show_result:
      X.info()
      print(X.head(), X.shape)
      y.info()
      print(y.head(), y.shape)
    return X, y
  
# Test the function
X, y = series2seq(df_test, n_in=2, n_out=1,labels = target_names,series_features=series_feature_names, show_result=True)

"""### `treatment_cars` Function to prepare the data set for each car"""

# Test data frame
# data set of the first 5 vehicles
df_test = df[df.Vehicle_ID.isin(vehicle_ids[:5])].copy()
df_test.info()
print(df_test.Vehicle_ID.unique())
df_test.head()

def treatment_cars(data, n_in=1, n_out=1,labels=None,series_features=None, show_result=False):
  veh_ids = data.Vehicle_ID.unique()
  dat_X, dat_y = pd.DataFrame(),pd.DataFrame()

  for id in veh_ids:
    dat = data[data.Vehicle_ID==id].copy()
    X, y = series2seq(dat.drop(columns=['Frame_ID']), n_in=n_in, n_out=n_out,labels = labels,series_features=series_features)
    dat_X = pd.concat([dat_X,X],ignore_index=True)
    dat_y = pd.concat([dat_y,y],ignore_index=True)
  if show_result:
    dat_X.info()
    print(dat_X.head(), dat_X.shape)
    dat_y.info()
    print(dat_y.head(), dat_y.shape)
  return dat_X ,dat_y
treatment_cars(df_test,n_in=2, n_out=1, labels = target_names,series_features=series_feature_names, show_result=True)

"""### Choose the size of the raw data set"""

np.random.seed(23)
new_size = 20
veh_list = np.random.choice(df.Vehicle_ID.unique(),new_size)
sub_df = df[df.Vehicle_ID.isin(veh_list)].copy()
sub_df.info()
sub_df.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# X, y = treatment_cars(sub_df, 
#                    n_in=n_steps, n_out=n_future,
#                    labels = target_names,
#                    series_features=series_feature_names, show_result=True)

"""### Split the data set"""

X_train, X_test, y_train, y_test = train_test_split(X,y, 
                                                    test_size=0.3, random_state=42)
print(X_train.shape,X_test.shape, y_train.shape, y_test.shape)

X_train.describe()

### Standardize the data
train_mean = X_train.mean()
train_std = X_train.std()

X_train = (X_train - train_mean) / train_std
X_test = (X_test - train_mean) / train_std

print(X_train.shape)
X_train.describe()

"""### Reshape data sets"""

X_train = X_train.values
X_test = X_test.values
# reshape into [# samples, # timesteps,# features]
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1],1))
X_test = X_test.reshape((X_test.shape[0], X_train.shape[1],1))

"""## Prediction model"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # define model
# model = Sequential()
# model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1],1)))
# model.add(Dense(n_labels*n_future))
# model.compile(optimizer='adam', loss='mse', metrics=['mse'])
# 
# # For saving the best model during the whole training process.
# #checkpointer = callbacks.ModelCheckpoint(filepath='BestModel.h5', monitor='val_loss', save_best_only=True)
# 
# #### Interrupt training if `val_loss` stops improving for over 10 epochs #######
# stop_learn= tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss')
# 
# 
# # fit model
# Monitor = model.fit(X_train, y_train, epochs=50, 
#                     callbacks=[stop_learn],
#                     validation_data=(X_test, y_test), verbose=1)

hist = pd.DataFrame(Monitor.history)
hist['epoch'] = Monitor.epoch
fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10,4),dpi=150)
hist[['loss','val_loss']].plot(ax=axes[0])
hist[['mse','val_mse']].plot(ax=axes[1])
plt.show()
hist.tail()

"""## Evaluation"""

yhat = model.predict(X_test, verbose=1)
rms = [sqrt(mean_squared_error(y_test, yhat))]
print(yhat[:5])
rms

plt.figure(figsize=(n_labels*7,n_future*5))
k = 0
for i in range(n_future):
  for j in range(n_labels):
    plt.subplot(n_future,n_labels,k+1)
    plt.scatter(y_test.index,y_test.iloc[:,k], label = "true {} at t+{}".format(target_names[j],i),marker = 'X', )
    plt.scatter(y_test.index,yhat[:,k], label = "prediction {} at t+{}".format(target_names[j],i),marker = '.')
    plt.legend()
    k+=1
plt.show()

"""# Model 2 With PCA

## Data preparation

### Choose the size of the raw data set
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# X, y = treatment_cars(sub_df, 
#                    n_in=n_steps, n_out=n_future,
#                    labels = target_names,
#                    series_features=series_feature_names, show_result=True)

"""### Split the data set"""

X_train, X_test, y_train, y_test = train_test_split(X,y, 
                                                    test_size=0.3, random_state=42)
print(X_train.shape,X_test.shape, y_train.shape, y_test.shape)

X_train.describe()

### Standardize the data
train_mean = X_train.mean()
train_std = X_train.std()

X_train = (X_train - train_mean) / train_std
X_test = (X_test - train_mean) / train_std

print(X_train.shape)
X_train.describe()

"""## PCA"""

from sklearn.decomposition import PCA

pca = PCA(n_components=12)
pca.fit(X_train)
plt.plot(pca.explained_variance_ratio_)
plt.show()
plt.plot(pca.singular_values_)

X_train = pca.transform(X_train)
X_test = pca.transform(X_test)

print(pca.singular_values_)

"""### Reshape data sets"""

#X_train = X_train.values
#X_test = X_test.values
# reshape into [# samples, # timesteps,# features]
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1],1))
X_test = X_test.reshape((X_test.shape[0], X_train.shape[1],1))

"""## Prediction model"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # define model
# model = Sequential()
# model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1],1)))
# model.add(Dense(n_labels*n_future))
# model.compile(optimizer='adam', loss='mse', metrics=['mse'])
# 
# # For saving the best model during the whole training process.
# #checkpointer = callbacks.ModelCheckpoint(filepath='BestModel.h5', monitor='val_loss', save_best_only=True)
# 
# #### Interrupt training if `val_loss` stops improving for over 10 epochs #######
# stop_learn= tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss')
# 
# 
# # fit model
# Monitor = model.fit(X_train, y_train, epochs=50, 
#                     callbacks=[stop_learn],
#                     validation_data=(X_test, y_test), verbose=1)

hist = pd.DataFrame(Monitor.history)
hist['epoch'] = Monitor.epoch
fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10,4),dpi=150)
hist[['loss','val_loss']].plot(ax=axes[0])
hist[['mse','val_mse']].plot(ax=axes[1])
plt.show()
hist.tail()

"""## Evaluation"""

yhat = model.predict(X_test, verbose=1)
rms = [sqrt(mean_squared_error(y_test, yhat))]
print(yhat[:5])
rms

plt.figure(figsize=(n_labels*7,n_future*5))
k = 0
for i in range(n_future):
  for j in range(n_labels):
    plt.subplot(n_future,n_labels,k+1)
    plt.scatter(y_test.index,y_test.iloc[:,k], label = "true {} at t+{}".format(target_names[j],i),marker = 'X', )
    plt.scatter(y_test.index,yhat[:,k], label = "prediction {} at t+{}".format(target_names[j],i),marker = '.')
    plt.legend()
    k+=1
plt.show()

"""# Various models (3 → 7)"""

LSTM_names = ['Vanilla LSTM',
              'Stacked LSTM',
              'Bidirectional',
              'CNN LSTM',
              'Conv LSTM',
              'Encoder-Decoder LSTM']

# Function to create simple LSTM models
def create_model(LSTM_name):
  # Create a new model
  model = Sequential()

  if LSTM_name == LSTM_names[0]:    
    model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1],1)))
    model.add(Dense(n_labels*n_future))

  elif LSTM_name == LSTM_names[1]:
    # Stacked LSTM
    model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(X_train.shape[1],1)))
    model.add(LSTM(50, activation='relu'))
    model.add(Dense(n_labels*n_future))
  
  elif LSTM_name == LSTM_names[2]:
    # Bidirectional
    model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(X_train.shape[1],1)))
    model.add(Dense(n_labels*n_future))

  elif LSTM_name == LSTM_names[3]:
    # CNN LSTM
    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, 4,1)))
    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))
    model.add(TimeDistributed(Flatten()))
    model.add(LSTM(50, activation='relu'))
    model.add(Dense(n_labels*n_future))

  elif LSTM_name == LSTM_names[4]:
    # Conv LSTM
    model.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', input_shape=(3, 1, 4,1)))
    model.add(Flatten())
    model.add(Dense(n_labels*n_future))

  elif LSTM_name == LSTM_names[5]:
    # Encoder-Decoder LSTM.
    model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1],1)))
    model.add(RepeatVector(n_labels*n_future))
    model.add(LSTM(50, activation='relu', return_sequences=True))
    model.add(TimeDistributed(Dense(1)))
  # compile the model
  model.compile(optimizer='adam', loss='mse', metrics=['mse'])
  return model

stop_learn= tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss')

"""## Prediction model"""

# for model ConvLSTM
X_train = X_train.reshape((X_train.shape[0],3,1,4,1))
X_test = X_test.reshape((X_test.shape[0],3,1,4,1))
X_train.shape
# = X.reshape((X.shape[0], n_seq, n_steps, n_features))

X_train.shape

# for model E-D LSTM
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1],1))
X_test = X_test.reshape((X_test.shape[0], X_train.shape[1],1))
X_train.shape

y_train = y_train.reshape((y_train.shape[0], y_train.shape[1]))
y_test = y_test.reshape((y_test.shape[0], y_train.shape[1]))
y_train.shape

model.summary()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # create model
# model = create_model(LSTM_names[5])
# 
# # fit model
# Monitor = model.fit(X_train, y_train, epochs=50, 
#                     callbacks=[stop_learn],
#                     validation_data=(X_test, y_test), verbose=1)
# # Check training process
# hist = pd.DataFrame(Monitor.history)
# hist['epoch'] = Monitor.epoch
# fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10,4),dpi=150)
# hist[['loss','val_loss']].plot(ax=axes[0])
# hist[['mse','val_mse']].plot(ax=axes[1])
# plt.show()
# hist.tail()

"""## Evaluation"""

yhat.shape

yhat = model.predict(X_test, verbose=1)

yhat = yhat.reshape((yhat.shape[0], yhat.shape[1]))
rms += [sqrt(mean_squared_error(y_test, yhat))]
print(yhat[:5])
rms

plt.figure(figsize=(n_labels*7,n_future*5))
k = 0
for i in range(n_future):
  for j in range(n_labels):
    plt.subplot(n_future,n_labels,k+1)
    plt.scatter(y_test.index,y_test.iloc[:,k], label = "true {} at t+{}".format(target_names[j],i),marker = 'X', )
    plt.scatter(y_test.index,yhat[:,k], label = "prediction {} at t+{}".format(target_names[j],i),marker = '.')
    plt.legend()
    k+=1
plt.show()

"""#END"""